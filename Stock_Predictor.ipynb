{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Stock Prediction Project\n",
        "The Multivariate Time Series Forecasting project leverages Python and PyTorch and other essential libraries for data analysis and deep learning. The lab covers key steps, including data loading from Yahoo Finance, exploratory data analysis, preprocessing, model training, and performance evaluation. By focusing on the NASDAQ stock market index, the tutorial demonstrates the application of advanced techniques in time series forecasting. Readers are taken through the process of building, training, and evaluating a model, with a practical example predicting the next day's stock price.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "doV986bRyppN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make a copy of this notebook! Editing directly will not be saved."
      ],
      "metadata": {
        "id": "th9QMUTDBavx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl67nJVhNJge"
      },
      "source": [
        "# Step #1 Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import date, timedelta, datetime\n",
        "from pandas.plotting import register_matplotlib_converters\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import yfinance as yf\n",
        "sns.set_style('white', { 'axes.spines.right': False, 'axes.spines.top': False})"
      ],
      "metadata": {
        "id": "Y2cQNjNGGTGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the timeframe for the data extraction\n",
        "end_date =  date.today().strftime(\"%Y-%m-%d\")\n",
        "start_date = '2010-01-01'\n",
        "\n",
        "# Getting NVIDIA quotes\n",
        "stockname = 'NVIDIA'\n",
        "symbol = 'NVDA'\n",
        "df = yf.download(symbol, start=start_date, end=end_date)"
      ],
      "metadata": {
        "id": "qZZicUjQz9Hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PASRXWD4NJgk"
      },
      "source": [
        "# Step #2 Exploring the Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "sIXu1zMBz-es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot line charts\n",
        "df_plot = df.copy()\n",
        "\n",
        "ncols = 2\n",
        "nrows = int(round(df_plot.shape[1] / ncols, 0))\n",
        "\n",
        "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, sharex=True, figsize=(14, 7))\n",
        "for i, ax in enumerate(fig.axes):\n",
        "        sns.lineplot(data = df_plot.iloc[:, i], ax=ax)\n",
        "        ax.tick_params(axis=\"x\", rotation=30, labelsize=10, length=0)\n",
        "        ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V7SY9YASGYv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skkWybexNJgl"
      },
      "source": [
        "# Step #3 Preprocessing and Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Indexing Batches\n",
        "train_df = df.sort_values(by=['Date']).copy()\n",
        "\n",
        "# List of considered Features\n",
        "FEATURES = ['High', 'Low', 'Open', 'Close', 'Volume']\n",
        "\n",
        "print('FEATURE LIST')\n",
        "print([f for f in FEATURES])\n"
      ],
      "metadata": {
        "id": "j8gw5ZyfGcv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the dataset with features and filter the data to the list of FEATURES\n",
        "data = pd.DataFrame(train_df)\n",
        "data_filtered = data[FEATURES]\n",
        "\n",
        "# We add a prediction column and set dummy values to prepare the data for scaling\n",
        "data_filtered_ext = data_filtered.copy()\n",
        "data_filtered_ext['Prediction'] = data_filtered_ext['Close']\n",
        "\n",
        "data_filtered_ext.tail()"
      ],
      "metadata": {
        "id": "rxENQPJU0D_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the number of rows in the data\n",
        "nrows = data_filtered.shape[0]\n",
        "\n",
        "# Convert the data to numpy values\n",
        "np_data_unscaled = np.array(data_filtered)\n",
        "np_data = np.reshape(np_data_unscaled, (nrows, -1))\n",
        "print(np_data.shape)\n",
        "\n",
        "# Transform the data by scaling each feature to a range between 0 and 1\n",
        "scaler = MinMaxScaler()\n",
        "np_data_scaled = scaler.fit_transform(np_data_unscaled)\n",
        "\n",
        "# Creating a separate scaler that works on a single column for scaling predictions\n",
        "scaler_pred = MinMaxScaler()\n",
        "df_Close = pd.DataFrame(data_filtered_ext['Close'])\n",
        "np_Close_scaled = scaler_pred.fit_transform(df_Close)"
      ],
      "metadata": {
        "id": "JyzkMm-AGfee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the sequence length - this is the timeframe used to make a single prediction\n",
        "sequence_length = 50\n",
        "\n",
        "# Prediction Index\n",
        "index_Close = data.columns.get_loc(\"Close\")\n",
        "\n",
        "# Split the training data into train and train data sets\n",
        "# As a first step, we get the number of rows to train the model on 80% of the data\n",
        "train_data_len = math.ceil(np_data_scaled.shape[0] * 0.8)\n",
        "\n",
        "# Create the training and test data\n",
        "train_data = np_data_scaled[0:train_data_len, :]\n",
        "test_data = np_data_scaled[train_data_len - sequence_length:, :]\n",
        "\n",
        "# The RNN needs data with the format of [samples, time steps, features]\n",
        "# Here, we create N samples, sequence_length time steps per sample, and 6 features\n",
        "def partition_dataset(sequence_length, data):\n",
        "    x, y = [], []\n",
        "    data_len = data.shape[0]\n",
        "    for i in range(sequence_length, data_len):\n",
        "        x.append(data[i-sequence_length:i,:]) #contains sequence_length values 0-sequence_length * columsn\n",
        "        y.append(data[i, index_Close]) #contains the prediction values for validation,  for single-step prediction\n",
        "\n",
        "    # Convert the x and y to numpy arrays\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    return x, y\n",
        "\n",
        "# Generate training data and test data\n",
        "x_train, y_train = partition_dataset(sequence_length, train_data)\n",
        "x_test, y_test = partition_dataset(sequence_length, test_data)\n",
        "\n",
        "# Print the shapes: the result is: (rows, training_sequence, features) (prediction value, )\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)\n",
        "\n",
        "# Validate that the prediction value and the input match up\n",
        "# The last close price of the second input sample should equal the first prediction value\n",
        "print(x_train[1][sequence_length-1][index_Close])\n",
        "print(y_train[0])"
      ],
      "metadata": {
        "id": "pyyWnLtUGjQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLzJnWK2NJgo"
      },
      "source": [
        "# Step #4 Model Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import math"
      ],
      "metadata": {
        "id": "UjKkiRVhKlQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc1 = nn.Linear(hidden_size, 5)\n",
        "        self.fc2 = nn.Linear(5, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc1(out[:, -1, :])\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "iDlXH3Zd6QCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing steps (assuming data_filtered, and the train and test splits are done)\n",
        "scaler = MinMaxScaler()\n",
        "np_data_unscaled = np.array(data_filtered)\n",
        "np_data_scaled = scaler.fit_transform(np_data_unscaled)\n",
        "\n",
        "x_train, y_train = partition_dataset(sequence_length, train_data)  # Assuming partition_dataset is defined\n",
        "x_test, y_test = partition_dataset(sequence_length, test_data)\n",
        "\n",
        "n_neurons = x_train.shape[1] * x_train.shape[2]\n",
        "input_size = x_train.shape[2]\n",
        "hidden_size = n_neurons // 2\n",
        "num_layers = 2\n",
        "output_size = 1\n",
        "learning_rate = 1e-4\n",
        "\n",
        "model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "sHJzDv8k0lnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to Torch tensors\n",
        "x_train = torch.from_numpy(x_train).float()\n",
        "y_train = torch.from_numpy(y_train).float()\n",
        "x_test = torch.from_numpy(x_test).float()\n",
        "y_test = torch.from_numpy(y_test).float()\n",
        "\n",
        "train_dataset = TimeSeriesDataset(x_train, y_train)\n",
        "test_dataset = TimeSeriesDataset(x_test, y_test)\n",
        "\n",
        "batch_size = 16\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for x_batch, y_batch in train_dataloader:\n",
        "        outputs = model(x_batch)\n",
        "        loss = criterion(outputs, y_batch.unsqueeze(1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Get the predicted values\n",
        "y_pred_scaled = model(x_test)\n",
        "y_pred_scaled_numpy = y_pred_scaled.detach().numpy()\n",
        "\n",
        "# Unscale the predicted values\n",
        "y_pred = scaler_pred.inverse_transform(y_pred_scaled_numpy)\n",
        "y_test_unscaled = scaler_pred.inverse_transform(y_test.reshape(-1, 1))\n"
      ],
      "metadata": {
        "id": "tGjHZYtC0qw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPo-zBEaNJgp"
      },
      "source": [
        "# Step #5 Evaluate Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAE = mean_absolute_error(y_test_unscaled, y_pred)\n",
        "print(f'Median Absolute Error (MAE): {np.round(MAE, 2)}')\n",
        "\n",
        "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
        "print(f'Mean Absolute Percentage Error (MAPE): {np.round(MAPE, 2)} %')\n",
        "\n",
        "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled)) ) * 100\n",
        "print(f'Median Absolute Percentage Error (MDAPE): {np.round(MDAPE, 2)} %')"
      ],
      "metadata": {
        "id": "hIIWFXuK0u_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The date from which on the date is displayed\n",
        "display_start_date = \"2019-01-01\"\n",
        "\n",
        "# Add the difference between the valid and predicted prices\n",
        "train = pd.DataFrame(data_filtered_ext['Close'][:train_data_len + 1]).rename(columns={'Close': 'y_train'})\n",
        "valid = pd.DataFrame(data_filtered_ext['Close'][train_data_len:]).rename(columns={'Close': 'y_test'})\n",
        "valid.insert(1, \"y_pred\", y_pred, True)\n",
        "valid.insert(1, \"residuals\", valid[\"y_pred\"] - valid[\"y_test\"], True)\n",
        "df_union = pd.concat([train, valid])\n",
        "\n",
        "# Zoom in to a closer timeframe\n",
        "df_union_zoom = df_union[df_union.index > display_start_date]\n",
        "\n",
        "# Create the lineplot\n",
        "fig, ax1 = plt.subplots(figsize=(16, 8))\n",
        "plt.title(\"y_pred vs y_test\")\n",
        "plt.ylabel(stockname, fontsize=18)\n",
        "sns.set_palette([\"#090364\", \"#1960EF\", \"#EF5919\"])\n",
        "sns.lineplot(data=df_union_zoom[['y_pred', 'y_train', 'y_test']], linewidth=1.0, dashes=False, ax=ax1)\n",
        "\n",
        "# Create the bar plot with the differences\n",
        "df_sub = [\"#2BC97A\" if x > 0 else \"#C92B2B\" for x in df_union_zoom[\"residuals\"].dropna()]\n",
        "ax1.bar(height=df_union_zoom['residuals'].dropna(), x=df_union_zoom['residuals'].dropna().index, width=3, label='residuals', color=df_sub)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YQpXpECbHwJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKZ6oZ_5NJgq"
      },
      "source": [
        "# Step #6 Predict Next Day's Price"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_temp = df[-sequence_length:]\n",
        "new_df = df_temp.filter(FEATURES)\n",
        "\n",
        "N = sequence_length\n",
        "\n",
        "# Get the last N day closing price values and scale the data to be values between 0 and 1\n",
        "last_N_days = new_df[-sequence_length:].values\n",
        "last_N_days_scaled = scaler.transform(last_N_days)\n",
        "\n",
        "# Create an empty list and Append past N days\n",
        "X_test_new = []\n",
        "X_test_new.append(last_N_days_scaled)\n",
        "\n",
        "# Convert the X_test data set to a numpy array and reshape the data\n",
        "X_test_new = torch.from_numpy(np.array(X_test_new)).float()\n",
        "\n",
        "# Make predictions using the model\n",
        "pred_price_scaled = model(X_test_new)\n",
        "pred_price_unscaled = scaler_pred.inverse_transform(pred_price_scaled.detach().numpy().reshape(-1, 1))\n",
        "\n",
        "# Print last price and predicted price for the next day\n",
        "price_today = np.round(new_df['Close'][-1], 2)\n",
        "predicted_price = np.round(pred_price_unscaled.ravel()[0], 2)\n",
        "change_percent = np.round(100 - (price_today * 100)/predicted_price, 2)\n",
        "\n",
        "plus = '+'; minus = ''\n",
        "print(f'The close price for {stockname} at {end_date} was {price_today}')\n",
        "print(f'The predicted close price is {predicted_price} ({plus if change_percent > 0 else minus}{change_percent}%)')"
      ],
      "metadata": {
        "id": "-Rd6nxYF1ZfK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}